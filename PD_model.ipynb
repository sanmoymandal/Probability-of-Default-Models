{
  "metadata": {
    "language_info": {
      "name": ""
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import piplite\nawait piplite.install('seaborn')",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, confusion_matrix\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport seaborn as sns",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Generate synthetic data\nnp.random.seed(42)\ndata_size = 1000\npayment_history = np.random.normal(0.5, 1, data_size)\ncredit_utilization = np.random.uniform(0.3, 0.9, data_size)\ncredit_history_length = np.random.normal(0.5, 1, data_size)\ntotal_accounts = np.random.uniform(0.1, 1, data_size)\ncredit_score = (0.4 * payment_history + 0.4 * credit_utilization + 0.2 * credit_history_length + 0.1 * total_accounts) * 900\ndefault = np.random.choice([0, 1], size=(data_size,), p=[0.8, 0.2])\n\ndf = pd.DataFrame({\n    'Payment History': payment_history,\n    'Credit Utilization': credit_utilization,\n    'Credit History Length': credit_history_length,\n    'Total Accounts': total_accounts,\n    'Credit Score': credit_score,\n    'Default': default\n})\n\n# Features and target variable\nX = df[['Payment History', 'Credit Utilization', 'Credit History Length', 'Total Accounts', 'Credit Score']]\ny = df['Default']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "class PDModel:\n    def __init__(self, model, model_name):\n        self.model = model\n        self.model_name = model_name\n        self.y_pred = None\n        self.y_prob = None\n\n    def train_model(self, X_train, y_train):\n        self.model.fit(X_train, y_train)\n\n    def predict(self, X_test):\n        self.y_pred = self.model.predict(X_test)\n        self.y_prob = self.model.predict_proba(X_test)[:, 1]\n\n    def evaluate_model(self, y_true):\n        accuracy = accuracy_score(y_true, self.y_pred)\n        # Calculate precision, recall, and f1 only if there are positive predictions\n        if np.sum(self.y_pred == 1) > 0:\n            precision = precision_score(y_true, self.y_pred)\n            recall = recall_score(y_true, self.y_pred)\n            f1 = f1_score(y_true, self.y_pred)\n        else:\n            precision, recall, f1 = 0, 0, 0\n        roc_auc = roc_auc_score(y_true, self.y_prob)\n\n        print(f\"\\nModel: {self.model_name}\")\n        print(f\"Accuracy: {accuracy:.4f}\")\n        print(f\"Precision: {precision:.4f}\")\n        print(f\"Recall: {recall:.4f}\")\n        print(f\"F1 Score: {f1:.4f}\")\n        print(f\"ROC AUC: {roc_auc:.4f}\")\n\n    def plot_roc_curve(self, y_true):\n        fpr, tpr, _ = roc_curve(y_true, self.y_prob)\n        plt.figure(figsize=(8, 6))\n        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'{self.model_name} (AUC = {roc_auc_score(y_true, self.y_prob):.2f})')\n        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title('Receiver Operating Characteristic (ROC) Curve')\n        plt.legend()\n        plt.show()\n\n    def plot_confusion_matrix(self, y_true):\n        cm = confusion_matrix(y_true, self.y_pred)\n        plt.figure(figsize=(6, 4))\n        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n        plt.title(f'Confusion Matrix - {self.model_name}')\n        plt.xlabel('Predicted')\n        plt.ylabel('True')\n        plt.show()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "class CreditScorePDModel:\n    def __init__(self):\n        self.mean_defaults = {}\n        self.y_pred = None\n        self.y_prob = None\n        self.model_name = 'Credit Score PD Model'\n\n    def train_model(self, X_train, y_train):\n        df = X_train.copy()\n        df['Default'] = y_train\n        grouped_data = df.groupby('Credit Score')['Default'].mean()\n        self.mean_defaults = grouped_data.to_dict()\n\n    def predict_proba(self, X):\n        x_values = X['Credit Score']\n        prob_default = np.array([self.mean_defaults.get(score, 0) for score in x_values])\n        self.y_prob = prob_default\n        self.y_pred = np.where(prob_default >= 0.5, 1, 0)\n        return prob_default\n\n    def evaluate_model(self, y_true):\n        accuracy = np.mean(self.y_pred == y_true)\n        precision = np.sum((self.y_pred == 1) & (y_true == 1)) / np.sum(self.y_pred == 1) if np.sum(self.y_pred == 1) > 0 else 0\n        recall = np.sum((self.y_pred == 1) & (y_true == 1)) / np.sum(y_true == 1)\n        f1 = 2 * (precision * recall) / (precision + recall)\n        roc_auc = roc_auc_score(y_true, self.y_prob)\n\n        print(f\"\\nModel: {self.model_name}\")\n        print(f\"Accuracy: {accuracy:.4f}\")\n        print(f\"Precision: {precision:.4f}\")\n        print(f\"Recall: {recall:.4f}\")\n        print(f\"F1 Score: {f1:.4f}\")\n        print(f\"ROC AUC: {roc_auc:.4f}\")\n\n    def plot_roc_curve(self, y_true):\n        fpr, tpr, _ = roc_curve(y_true, self.y_prob)\n        plt.figure(figsize=(8, 6))\n        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'{self.model_name} (AUC = {roc_auc_score(y_true, self.y_prob):.2f})')\n        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title('Receiver Operating Characteristic (ROC) Curve')\n        plt.legend()\n        plt.show()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Create and train models\nlogreg_model = LogisticRegression()\ndt_model = DecisionTreeClassifier(random_state=42)\ncredit_score_pd_model = CreditScorePDModel()\n\nlogreg_pd_model = PDModel(logreg_model, 'Logistic Regression')\ndt_pd_model = PDModel(dt_model, 'Decision Tree')\n\n# Train and evaluate Logistic Regression model\nlogreg_pd_model.train_model(X_train, y_train)\nlogreg_pd_model.predict(X_test)\nlogreg_pd_model.evaluate_model(y_test)\nlogreg_pd_model.plot_roc_curve(y_test)\nlogreg_pd_model.plot_confusion_matrix(y_test)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Train and evaluate Decision Tree model\ndt_pd_model.train_model(X_train, y_train)\ndt_pd_model.predict(X_test)\ndt_pd_model.evaluate_model(y_test)\ndt_pd_model.plot_roc_curve(y_test)\ndt_pd_model.plot_confusion_matrix(y_test)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Train and evaluate Credit Score PD Model\ncredit_score_pd_model = CreditScorePDModel()\ncredit_score_pd_model.train_model(X_train, y_train)\n\n# Predict probabilities on the test set\ncredit_score_pd_model.predict_proba(X_test)\n\n# Evaluate the model\ncredit_score_pd_model.evaluate_model(y_test)\n\n# Plot ROC curve\ncredit_score_pd_model.plot_roc_curve(y_test)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Visualizations\ndef plot_scatter(x, y, title, xlabel, ylabel):\n    plt.scatter(x, y, alpha=0.5)\n    plt.title(title)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.show()\n\ndef plot_distribution(data, title, xlabel):\n    sns.histplot(data, kde=True)\n    plt.title(title)\n    plt.xlabel(xlabel)\n    plt.show()\n\ndef plot_pair_plot(data, hue, title):\n    sns.pairplot(data, hue=hue)\n    plt.suptitle(title, y=1.02)\n    plt.show()\n\n# Scatter Plot\nplot_scatter(df['Credit Score'], df['Credit Utilization'], 'Credit Score vs Credit Utilization', 'Credit Score', 'Credit Utilization')\n\n# Distribution Plot\nplot_distribution(df['Credit Score'], 'Distribution of Credit Score', 'Credit Score')\n\n# Pair Plot\nplot_pair_plot(df, hue='Default', title='Pair Plot')",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}